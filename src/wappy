#!/usr/bin/env python

import sys
import argparse
import requests
from colorama import Fore, Back, Style
import warnings
import os
import concurrent
import json
import time
import pkg_resources
warnings.filterwarnings("ignore")

#check that the proper version of Wappalyzer is installed
try:
    pkg_resources.require("python-Wappalyzer>=0.4.0")
except pkg_resources.VersionConflict as e:
    print(Style.BRIGHT+Fore.RED,"\nError:",e,Style.BRIGHT+Fore.BLUE,"""
    
Wappalyzer Version is unsupported, likely installed by pip. Upgrade it with the following commands:""",Style.RESET_ALL,"""

pip uninstall python-Wappalyzer || sudo pip uninstall python-Wappalyzer
git clone https://github.com/chorsley/python-Wappalyzer.git /tmp/python-Wappalyzer
cd /tmp/python-Wappalyzer/
sudo python setup.py install""")
    sys.exit()
    
except Exception as e:
    print(Style.BRIGHT+Fore.RED,"\nError:",e,Style.BRIGHT+Fore.BLUE,"""

Install it with the following commands:""",Style.RESET_ALL,"""
  
git clone https://github.com/chorsley/python-Wappalyzer.git /tmp/python-Wappalyzer
cd /tmp/python-Wappalyzer
sudo python setup.py install""")
    sys.exit()
    
from Wappalyzer import Wappalyzer, WebPage    

def update_technologies():
  print("updating technologies",end="")
  _technologies_file = os.path.expanduser('~/.python-Wappalyzer/technologies.json')
  cats = requests.get('https://github.com/AliasIO/wappalyzer/raw/master/src/categories.json').json()
  techs = {}
  #for _ in '_abcdefghijklmnopqrstuvwxyz':
  for _ in '_abcdefghijklmnopqrstuvwxyz':
    r = requests.get(f'https://github.com/AliasIO/wappalyzer/raw/master/src/technologies/{_}.json')
    print(".",end ="")
    techs = {**techs, **r.json()}
  obj = {'categories': cats, 'technologies': techs}
  print()

  with open(_technologies_file, 'w', encoding='utf-8') as tfile:
    tfile.write(json.dumps(obj))


def find_version(a):
  if a == []:
   return 'unknown'
  else:
   return a[0]
  
def handle_http_errors(address):
  try:
    return requests.head(address, timeout=10, allow_redirects=True).url
  except requests.exceptions.HTTPError:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Http Error:",address)
    return 0
  except requests.exceptions.ConnectionError:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Error Connecting:",address)
    return 0
  except requests.exceptions.Timeout:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Timeout Error:",address)
    return 0
  except requests.exceptions.RequestException:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Oops: Something Else",address)
    return 0
  
def verify_url(slug):
  t = 'https://'+slug
  url = handle_http_errors(t)
  if url == 0:
    t = 'http://'+slug
    url = handle_http_errors(t)
  return url 	

def find_techs(host):
   host=host.strip()
   if len(host) == 0:
     return
   url=host
   if '.' in url and 'http' not in url:
     url=verify_url(url)    
   if url != 0:  
     try:
       webpage = WebPage.new_from_url(url, verify=False, timeout=60)
       wappalyzer= Wappalyzer.latest(technologies_file=_technologies_file)
       techs = wappalyzer.analyze_with_versions_and_categories(webpage)
     except Exception as e:
       if silencio != True:
         print(Style.BRIGHT + Fore.RED + "\n[!] WAPPALYZER ERROR: " + url, Style.RESET_ALL + ":\n")
         print(e)
       return 0

     nurl = url.split("//")[1].rstrip("/")
     
     print("\n[+]",Style.BRIGHT + Fore.BLUE + "TECHNOLOGIES", Style.BRIGHT + Fore.GREEN + f"[{nurl.upper()}]", Style.RESET_ALL + ":\n")

     for i in techs:
       if find_version(techs[i]['versions']) == "unknown":
         print(f"{techs[i]['categories'][0]} : {i}")
       else:
         print(f"{techs[i]['categories'][0]} : {i} [version: {find_version(techs[i]['versions'])}]")
       if j : 
         j.write(nurl.lower()+','+techs[i]['categories'][0]+','+i+','+find_version(techs[i]['versions'])+'\n')
     else:
       pass

     return 1
   else:
     return 0

parser = argparse.ArgumentParser(description='Finds Web Technologies !')
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('-u', '--url', help='url to find technologies')
group.add_argument('-f', '--file', default='', help="list of urls to find web technologies")
parser.add_argument('-wf', '--writefile', default='', help="File to write csv output to")
parser.add_argument('-t', '--threads', default='10', type=int, help="How many threads yo?")
parser.add_argument('-q', '--quiet', default='False', action='store_true', help="Don't want to see any errors?")

args = parser.parse_args()
threadcount=args.threads
url = args.url
file = args.file
writefile = args.writefile
silencio = args.quiet
_technologies_file = os.path.expanduser('~/.python-Wappalyzer/technologies.json')

if not os.path.exists(_technologies_file) or (os.path.getmtime(_technologies_file) < time.time() - 86400):
  update_technologies()

if writefile != '':
  j = open(writefile,'a')
  j.seek(0, os.SEEK_END)
  # if current position is not 0
  if j.tell():
    #rewind for future use
    j.seek(0)
  else:
    j.write('URL,CATEGORY,NAME,VERSION\n')
else:
  j=""
    
if file == '':
  pass
else:
  f = open(file, 'r')
  urls = f.readlines()
  with concurrent.futures.ThreadPoolExecutor(max_workers=threadcount) as executor:
    future_to_url = {executor.submit(find_techs, i): i for i in urls}
    for future in concurrent.futures.as_completed(future_to_url):
      url = future_to_url[future]
      try:
        future.result()
      except Exception as exc:
        print('%r generated an exception: %s' % (url, exc))

if url==None:
  pass
else:
  find_techs(url)
