#!/usr/bin/env python

import sys
import argparse
import requests
from colorama import Fore, Back, Style
import warnings
import os
import concurrent
import json
import time
import pkg_resources
from zipfile import ZipFile
import io
warnings.filterwarnings("ignore")


#check that the proper version of Wappalyzer is installed
try:
    pkg_resources.require("python-Wappalyzer>=0.4.0")
except pkg_resources.VersionConflict as e:
	#the pip version is out of date
    print(f"{Style.BRIGHT+Fore.RED}\nError: {e}")
    print(f"{Fore.BLUE}\nWappalyzer Version is unsupported, likely installed by pip. Upgrade it with the following commands: {Style.RESET_ALL}")
    print("""
pip uninstall python-Wappalyzer -y || sudo pip uninstall python-Wappalyzer -y
git clone https://github.com/chorsley/python-Wappalyzer.git /tmp/python-Wappalyzer
cd /tmp/python-Wappalyzer/
sudo python setup.py install
	""")
    sys.exit()

#sorry, can't set it using setup.py becuase the pip is too far behind
except Exception as e:
    print(f"{Style.BRIGHT+Fore.RED}\nError: {e}")
    print(f"{Fore.BLUE}\nInstall it with the following commands:{Style.RESET_ALL}")
    print("""
git clone https://github.com/chorsley/python-Wappalyzer.git /tmp/python-Wappalyzer
cd /tmp/python-Wappalyzer
sudo python setup.py install
	""")
    sys.exit()
    
#finally import
from Wappalyzer import Wappalyzer, WebPage    

def update_technologies():
    print("updating technologies", end="")

    technologies_file = os.path.expanduser('~/.python-Wappalyzer/technologies.json')
    #get new categories from current Wappalyzer repo
    categories_response = requests.get('https://github.com/AliasIO/wappalyzer/raw/master/src/categories.json')
    categories = categories_response.json()
    technologies = {}
    #get new technologies from current Wappalyzer repo. they have been split into multiple files
    for letter in '_abcdefghijklmnopqrstuvwxyz':
        technologies_response = requests.get(f'https://github.com/AliasIO/wappalyzer/raw/master/src/technologies/{letter}.json')
        print(".", end="")
        technologies = {**technologies, **technologies_response.json()}
    #merge into one object
    combined_object = {'categories': categories, 'technologies': technologies}
    print()

    with open(technologies_file, 'w', encoding='utf-8') as tfile:
        tfile.write(json.dumps(combined_object))
        tfile.flush()

def update_technologies_from_latest():
	print("updating technologies")
	technologies_file = os.path.expanduser('~/.python-Wappalyzer/technologies.json')
	technologies = {}
	 
	#get release page
	latest_release = requests.get('https://api.github.com/repos/wappalyzer/wappalyzer/releases/latest').json()
	#get zip from url
	zip_url = requests.get(latest_release['zipball_url'])
	myzip = ZipFile(io.BytesIO(zip_url.content)) 

	#parse files
	for listed_file in myzip.namelist():
		#get all technology files
		if "src/technologies" in listed_file and ".json" in listed_file:
			#extract file into json
			tech_json_file=myzip.read(listed_file).decode('UTF-8')
			#the latest release has an error
			if "t.json" in listed_file:
				tech_json_file = tech_json_file.replace('tally.so/embed/\'\"','tally.so/embed/\']\"')
			tech_json = json.loads(tech_json_file)
			#add to full json
			technologies = {**technologies, **tech_json}
		if "src/categories.json" in listed_file:
			#extract categories into json
			categories = json.loads(myzip.read(listed_file).decode('UTF-8'))
		#merge into one object
	combined_object = {'categories': categories, 'technologies': technologies}

	#write to file
	with open(technologies_file, 'w', encoding='utf-8') as tfile:
	    tfile.write(json.dumps(combined_object))
	    tfile.flush()
	print("done!\n")

def handle_http_errors(address):
	try:
		#tries to see if url is valid and allows for redirect
		return requests.head(address, verify=False, timeout=10, allow_redirects=True).url
	except requests.exceptions.HTTPError:
		return("Http Error")
	except requests.exceptions.ConnectionError:
		return("Error Connecting")
	except requests.exceptions.Timeout:
		return("Timeout Error")
	except requests.exceptions.RequestException:
		return("Some Other Error")
	else:
		return("Some Other Error")

def find_techs(host):
	host=host.strip()
	#for testing which protocols to use
	url_test=host
	
	if not host:
		return
	#checking input missing protocol and then trying to connect to https and then http
	if '.' in host and 'http' not in host:
		url_test = handle_http_errors('https://'+host)
	if "error" in url_test.lower():
		url_test = handle_http_errors('http://'+host)
	#if you cant connect to the IP with either protocol, raise the error
	if "error" in url_test.lower():
		raise ValueError(f"\n{Style.BRIGHT+Fore.RED+url_test}: {host} {Style.RESET_ALL}")
	else:
		#this is the one we will use
		final_url=url_test
	#try to scan the page using wappalyzer	
	try:
		webpage = WebPage.new_from_url(final_url, verify=False, timeout=60)
		wappalyzer= Wappalyzer.latest(technologies_file=technologies_file)
		techs = wappalyzer.analyze_with_versions_and_categories(webpage)
	except Exception as e:
		return f"{Style.BRIGHT+Fore.RED}\n[!] WAPPALYZER ERROR: {final_url}:\n{Style.RESET_ALL}\n{e}"

	#display purposes
	formatted_url = final_url.split("//")[1].rstrip("/")

	#instatiate output
	output=(f"{Style.BRIGHT + Fore.BLUE}\n[+] TECHNOLOGIES {Fore.GREEN} [{formatted_url.upper()}]{Style.RESET_ALL}")
	output_file_output=""

	for tech, tech_data in techs.items():
		#print technology
		output += f"\n{tech_data['categories'][0]} : {tech}"
		#if version number is empty change to unknown, else change it to the first response
		version = "unknown" if tech_data['versions'] == [] else tech_data['versions'][0]
		#add version to output
		if version != "unknown":
			output += f" [version: {version}]"
			
		#add technology to output file listing
		if output_file:	
			output_file_output+=f"{host},{final_url.lower()},{tech_data['categories'][0]},{tech},{version}\n"
	
	#write output to file if needed
	if output_file:
		output_file.write(output_file_output)
		output_file.flush()

	#returns output to end thread
	return(output)
       

parser = argparse.ArgumentParser(description='Finds Web Technologies!')
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('-u', '--url', help='url to find technologies')
group.add_argument('-f', '--file', default='', help="list of urls to find web technologies")
parser.add_argument('-wf', '--writefile', default='', help="File to write csv output to")
parser.add_argument('-t', '--threads', default=10, type=int, help="How many threads yo?")
parser.add_argument('-q', '--quiet', default=False, action='store_true', help="Don't want to see any errors?")

args = parser.parse_args()
thread_count=args.threads
suppress_errors = args.quiet

#get's the updated files
technologies_file = os.path.expanduser('~/.python-Wappalyzer/technologies.json')
if not os.path.exists(technologies_file) or os.path.getmtime(technologies_file) < time.time() - 86400:
	update_technologies_from_latest()

# Check if output file is specified
if args.writefile:
	# Open file in append mode and move cursor to the end
	output_file = open(args.writefile, 'a')
	output_file.seek(0, os.SEEK_END)

	# If current position is not 0, rewind the file for future use
	if output_file.tell():
		output_file.seek(0)
	else:
		# Write header to file
		output_file.write('URL,CATEGORY,NAME,VERSION\n')
else:
	output_file=""
    
# Check if input file is specified
if args.file:
	# Open file in read mode
	input_file = open(args.file, 'r')
	# Read all lines and store them in a list
	urls = input_file.readlines()

	# Use ThreadPoolExecutor to run the find_techs function concurrently
	with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:
		# Create a dictionary with a future object as key and the url as value
		future_to_url = {executor.submit(find_techs, i): i for i in urls}
		# Iterate through completed futures
		for future in concurrent.futures.as_completed(future_to_url):
			url = future_to_url[future]
			try:		
				print(future.result())
			except Exception as exc:
				#don't print errors if they don't ask
				if not suppress_errors:
					print(exc)

#runs a single instance
if args.url:
	try:		
		print(find_techs(args.url))
	except Exception as exc:
		#don't print errors if they don't ask
		if not suppress_errors:
			print(exc)
