#!/usr/bin/env python

from Wappalyzer import Wappalyzer, WebPage
import argparse
import requests
from colorama import Fore, Back, Style
import warnings
import os
import concurrent
warnings.filterwarnings("ignore")

def find_version(a):
  if a == []:
   return 'unknown'
  else:
   return a[0]
  
def handle_http_errors(address):
  try:
    return requests.head(address, timeout=10, allow_redirects=True).url
  except requests.exceptions.HTTPError:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Http Error:",address)
    return 0
  except requests.exceptions.ConnectionError:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Error Connecting:",address)
    return 0
  except requests.exceptions.Timeout:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Timeout Error:",address)
    return 0
  except requests.exceptions.RequestException:
    if silencio != True:
      print ('\n'+Style.BRIGHT+Fore.RED+"Oops: Something Else",address)
    return 0
  
def verify_url(slug):
  t = 'https://'+slug
  url = handle_http_errors(t)
  if url == 0:
    t = 'http://'+slug
    url = handle_http_errors(t)
  return url 	

def find_techs(host):
   host=host.strip()
   if len(host) == 0:
     return
   url=host
   if '.' in url and 'http' not in url:
     url=verify_url(url)    
   if url != 0:  
     try:
       webpage = WebPage.new_from_url(url, verify=False, timeout=60)
       wappalyzer = Wappalyzer.latest()
       techs = wappalyzer.analyze_with_versions_and_categories(webpage)
     except Exception as e:
       if silencio != True:
         print(Style.BRIGHT + Fore.RED + "\n[!] WAPPALYZER ERROR: " + url, Style.RESET_ALL + ":\n")
         print(e)
       return 0

     nurl = url.split("//")[1].rstrip("/")

     print("\n[+]",Style.BRIGHT + Fore.BLUE + "TECHNOLOGIES", Style.BRIGHT + Fore.GREEN + f"[{nurl.upper()}]", Style.RESET_ALL + ":\n")

     for i in techs:
       print(f"{techs[i]['categories'][0]} : {i} [version: {find_version(techs[i]['versions'])}]")
       if j : 
         j.write(nurl.lower()+','+techs[i]['categories'][0]+','+i+','+find_version(techs[i]['versions'])+'\n')
     else:
       pass

     return 1
   else:
     return 0

parser = argparse.ArgumentParser(description='Finds Web Technologies !')
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('-u', '--url', help='url to find technologies')
group.add_argument('-f', '--file', default='', help="list of urls to find web technologies")
parser.add_argument('-wf', '--writefile', default='', help="File to write csv output to")
parser.add_argument('-t', '--threads', default='10', type=int, help="How many threads yo?")
parser.add_argument('-q', '--quiet', default='False', action='store_true', help="Don't want to see any errors?")

args = parser.parse_args()
threadcount=args.threads
url = args.url
file = args.file
writefile = args.writefile
silencio = args.quiet

if writefile != '':
  j = open(writefile,'a')
  j.seek(0, os.SEEK_END)
  # if current position is not 0
  if j.tell():
    #rewind for future use
    j.seek(0)
  else:
    j.write('URL,CATEGORY,NAME,VERSION\n')
else:
  j=""
    
if file == '':
  pass
else:
  f = open(file, 'r')
  urls = f.readlines()
  with concurrent.futures.ThreadPoolExecutor(max_workers=threadcount) as executor:
    future_to_url = {executor.submit(find_techs, i): i for i in urls}
    for future in concurrent.futures.as_completed(future_to_url):
      url = future_to_url[future]
      try:
        future.result()
      except Exception as exc:
        print('%r generated an exception: %s' % (url, exc))

if url==None:
  pass
else:
  find_techs(url)
